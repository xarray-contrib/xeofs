{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you handle large datasets that exceed memory capacity, xeofs is designed to work with [dask](https://dask.org/)-backed\n",
    "xarray objects from end-to-end. By default, xeofs computes models eagerly, which in some\n",
    "cases can lead to better performance. However, it is also possible to build and fit models \"lazily\", meaning\n",
    "no computation will be carried out until the user calls ``.compute()``. To enable lazy computation, specify\n",
    "``compute=False`` when initializing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\\\"alert alert-block alert-info\\\">\n",
    "    <b>Note</b> <br>\n",
    "    Importantly, xeofs never loads the input dataset(s) into memory.</a>\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few tricks, and features that need to be explicitly disabled for lazy evaluation to work. First\n",
    "is the ``check_nans`` option, which skips checking for full or isolated ``NaNs`` in the data. In this case,\n",
    "the user is responsible for ensuring that the data is free of ``NaNs`` by first applying e.g. ``.dropna()``\n",
    "or ``.fillna()``. Second is that lazy mode is incompatible with assessing the fit of a rotator class during\n",
    "evaluation, becaue the entire ``dask`` task graph must be built up front. Therefore, a lazy rotator model will\n",
    "run out to the full ``max_iter`` regardless of the specified ``rtol``. For that reason it is recommended to\n",
    "reduce the number of iterations.\n",
    "\n",
    "As an example, the following lazily creates an EOF model for a 10GB dataset in about a second, which can\n",
    "then be evaluated later using ``.compute()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from dask.distributed import Client\n",
    "from xeofs.models import EOF, EOFRotator\n",
    "\n",
    "# Set up the LocalCluster\n",
    "client = Client(n_workers=4, threads_per_worker=1, memory_limit='3GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xr.DataArray(\n",
    "    da.random.random((5000, 720, 360), chunks=(100, 100, 100)),\n",
    "    dims=[\"time\", \"lon\", \"lat\"],\n",
    "    coords={\n",
    "        \"time\": xr.date_range(\"2000-01-01\", periods=5000, freq=\"D\"),\n",
    "        \"lon\": np.linspace(-180, 180, 720),\n",
    "        \"lat\": np.linspace(-90, 90, 360),\n",
    "    },\n",
    ")\n",
    "print(\"Size of Dataset (in GB): \", data.nbytes / 1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\\\"alert alert-block alert-warning\\\">\n",
    "    <b>Warning</b> <br>\n",
    "    Remember that dask allows you to compute out-of-memory datasets by writing intermediate results to your disk. However, computing a singular value decomposition (SVD) typically requires more memory during computation than the size of the input dataset. In this case, about twice the size of the dataset (~20 GB) was written to disk during the SVD computation. Usually, these results are written to /tmp/ on Linux machines. You can change the default directory by configuring dask, for example, using: `dask.config.set({'temporary_directory': '/your/temporary/directory'})`.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EOF(n_modes=5, check_nans=False, compute=False)\n",
    "model.fit(data, dim=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following, we can instruct dask to start the actual computation. A standard laptop (Intel(R) Core(TM) i7-8750H CPU @ 2.20GHz) with four cores, each using 3 GB of memory, needs about 15 minutes to compute the PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xeofs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
