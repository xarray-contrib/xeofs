{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Canonical Correlation Analysis\n",
    "\n",
    "In this example, we're going to perform a Canonical Correlation Analysis (CCA)\n",
    "on three datasets using the ERSSTv5 monthly sea surface temperature (SST) data\n",
    "from 1970 to 2022. We divide this data into three areas: the Indian Ocean,\n",
    "the Pacific Ocean, and the Atlantic Ocean. Our goal is to perform CCA on these\n",
    "regions.\n",
    "\n",
    "First, we'll import the necessary modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "import xeofs as xe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the data and compute the SST anomalies. This removes the\n",
    "monthly climatologies, so the seasonal cycle doesn't impact our CCA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst = xr.tutorial.load_dataset(\"ersstv5\").sst\n",
    "sst = sst.groupby(\"time.month\") - sst.groupby(\"time.month\").mean(\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define the three regions of interest and store them in a list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indian = sst.sel(lon=slice(35, 115), lat=slice(30, -30))\n",
    "pacific = sst.sel(lon=slice(130, 290), lat=slice(30, -30))\n",
    "atlantic = sst.sel(lon=slice(320, 360), lat=slice(70, 10))\n",
    "\n",
    "data_list = [indian, pacific, atlantic]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform CCA. Since we are dealing with a high-dimensional feature space, we first\n",
    "perform PCA to reduce the dimensionality (this is kind of a regularized CCA) by setting\n",
    "``pca=True``. By setting the ``variance_fraction`` keyword argument, we specify that we\n",
    "want to keep the number of PCA modes that explain 90% of the variance in each of the\n",
    "three data sets.\n",
    "\n",
    "An important parameter is ``init_pca_modes``. It specifies the number\n",
    "of PCA modes that are initially compute before truncating them to account for 90 %. If this\n",
    "number is small enough, randomized PCAs will be performed instead of the full SVD decomposition\n",
    "which is much faster. We can also specify ``init_pca_modes`` as a float (0 < x <= 1),\n",
    "in which case the number of PCA modes is given by the fraction of the data matrix's rank\n",
    "The default is set to 0.75 which will ensure that randomized PCAs are performed.\n",
    "\n",
    "Given the nature of SST data, we might lower it to something like 0.3, since\n",
    "we expect that most of the variance in the data will be explained by a small\n",
    "number of PC modes.\n",
    "\n",
    "Note that if our initial PCA modes don't hit the 90% variance target, ``xeofs``\n",
    "will give a warning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xe.multi.CCA(\n",
    "    n_modes=2,\n",
    "    use_coslat=True,\n",
    "    pca=True,\n",
    "    variance_fraction=0.9,\n",
    "    init_pca_modes=0.30,\n",
    ")\n",
    "model.fit(data_list, dim=\"time\")\n",
    "components = model.components()\n",
    "scores = model.scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the canonical loadings (components) of the first mode.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 1\n",
    "\n",
    "central_longitudes = [\n",
    "    indian.lon.median().item(),\n",
    "    pacific.lon.median().item(),\n",
    "    pacific.lon.median().item(),\n",
    "]\n",
    "projections = [ccrs.PlateCarree(central_longitude=lon) for lon in central_longitudes]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 2.5))\n",
    "gs = GridSpec(1, 4, figure=fig, width_ratios=[2, 4, 1, 0.2])\n",
    "axes = [fig.add_subplot(gs[0, i], projection=projections[i]) for i in range(3)]\n",
    "cax = fig.add_subplot(1, 4, 4)\n",
    "kwargs = dict(transform=ccrs.PlateCarree(), vmin=-1, vmax=1, cmap=\"RdBu_r\", cbar_ax=cax)\n",
    "components[0].sel(mode=mode).plot(ax=axes[0], **kwargs)\n",
    "components[1].sel(mode=mode).plot(ax=axes[1], **kwargs)\n",
    "im = components[2].sel(mode=mode).plot(ax=axes[2], **kwargs)\n",
    "fig.colorbar(im, cax=cax, orientation=\"vertical\")\n",
    "for ax in axes:\n",
    "    ax.coastlines()\n",
    "    ax.set_title(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly, we'll check out the canonical variates (scores) of the first mode.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "scores[0].sel(mode=mode).plot(ax=ax, label=\"Indian Ocean\")\n",
    "scores[1].sel(mode=mode).plot(ax=ax, label=\"Central Pacific\")\n",
    "scores[2].sel(mode=mode).plot(ax=ax, label=\"North Atlantic\")\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
